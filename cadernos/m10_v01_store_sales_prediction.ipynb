{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão da Linguagem Python Usada Neste Jupyter Notebook: 3.7.9\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print('Versão da Linguagem Python Usada Neste Jupyter Notebook:', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import requests\n",
    "import warnings\n",
    "import inflection\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n",
    "from scipy                 import stats  as ss\n",
    "from boruta                import BorutaPy\n",
    "from matplotlib            import pyplot as plt\n",
    "from IPython.display       import Image\n",
    "from IPython.core.display  import HTML\n",
    "\n",
    "\n",
    "from sklearn.metrics       import mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble      import RandomForestRegressor\n",
    "from sklearn.linear_model  import LinearRegression, Lasso\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, LabelEncoder\n",
    "\n",
    "warnings.filterwarnings( 'ignore' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramer_v (x,y):\n",
    "    cm= pd.crosstab(x,y).values\n",
    "    n=cm.sum()\n",
    "    r, k = cm.shape\n",
    "    \n",
    "    chi2 = ss.chi2_contingency(cm)[0]\n",
    "    chi2corr = max(0, chi2 - (k-1)*(r-1)/(n-1))\n",
    "    \n",
    "    kcorr = k - (k-1)**2/(n-1)\n",
    "    rcorr = r - (r-1)**2/(n-1)\n",
    "    return np.sqrt((chi2corr/n)/(min( kcorr-1, rcorr-1)))\n",
    "\n",
    "\n",
    "#Modelos de erro\n",
    "\n",
    "def mean_percentage_error( y, yhat ):\n",
    "    return np.mean( ( y - yhat ) / y )\n",
    "\n",
    "def mean_absolute_percentage_error(y,yhat):\n",
    "    return np.mean(np.abs((y-yhat)/y))\n",
    "\n",
    "def ml_error(model_name, y ,yhat):\n",
    "    mae = mean_absolute_error(y,yhat)\n",
    "    mape = mean_absolute_percentage_error(y,yhat)\n",
    "    rmse = np.sqrt(mean_squared_error(y,yhat))\n",
    "\n",
    "    return pd.DataFrame({'Model Name': model_name,\n",
    "                         'MAE': mae,\n",
    "                         'MAPE': mape,\n",
    "                         'RMSE': rmse}, index=[0])\n",
    "\n",
    "def cross_validation(x_training, kfold, model_name, model, verbose=False):\n",
    "          mae_list = []\n",
    "          mape_list= []\n",
    "          rmse_list= []\n",
    "\n",
    "          for k in reversed(range(1,kfold+1)):\n",
    "              if verbose:\n",
    "                print('\\nKfold Number: {}'.format(k))  \n",
    "              #start and end of validation\n",
    "              validation_start_date = x_training['date'].max() - timedelta(days=k*6*7)\n",
    "              validation_end_date = x_training['date'].max() - timedelta(days=k*6*7)\n",
    "\n",
    "              #filtering dataset \n",
    "              training= x_training[x_training['date'] < validation_start_date]\n",
    "              validation= x_training[ (x_training['date'] >= validation_start_date)  &  (x_training['date'] <= validation_end_date)  ]\n",
    "\n",
    "              #training and validation\n",
    "              #training\n",
    "              xtraining = training.drop(['date','sales'], axis=1)\n",
    "              ytraining = training['sales']\n",
    "\n",
    "              #validatiom \n",
    "              xvalidation = validation.drop(['date','sales'], axis=1)\n",
    "              yvalidation = validation['sales']\n",
    "\n",
    "              #model \n",
    "              m = model.fit(xtraining, ytraining)\n",
    "\n",
    "              #prediction\n",
    "              yhat =m.predict(xvalidation)\n",
    "\n",
    "              #performance\n",
    "              m_result = ml_error(model_name, np.expm1(yvalidation),np.expm1(yhat) ) \n",
    "              \n",
    "              #store performance of eack kfold iteration\n",
    "              mae_list.append (m_result['MAE'])\n",
    "              mape_list.append(m_result['MAPE'])\n",
    "              rmse_list.append(m_result['RMSE'])\n",
    "          return pd.DataFrame({'Model Name': model_name,\n",
    "                              'MAE CV': np.round(np.mean(mae_list), 2).astype(str) + '+/-' +  np.round(np.std(mae_list),2).astype(str),\n",
    "                              'MAPE CV': np.round(np.mean(mape_list),2).astype(str) + '+/-' + np.round(np.std(mape_list),2).astype(str),\n",
    "                              'RMSE CV': np.round(np.mean(rmse_list),2).astype(str) + '+/-' + np.round(np.std(rmse_list),2).astype(str)},index=[0])\n",
    "    \n",
    "def jupyter_settings():\n",
    "    %matplotlib inline\n",
    "    %pylab inline\n",
    "    \n",
    "    plt.style.use( 'bmh' )\n",
    "    plt.rcParams['figure.figsize'] = [25, 12]\n",
    "    plt.rcParams['font.size'] = 24\n",
    "    \n",
    "    display( HTML( '<style>.container { width:100% !important; }</style>') )\n",
    "    pd.options.display.max_columns = None\n",
    "    pd.options.display.max_rows = None\n",
    "    pd.set_option( 'display.expand_frame_repr', False )\n",
    "    \n",
    "    sns.set()\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jupyter_settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2. Carregando os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_raw = pd.read_csv( 'C:/Users/Michelle/repos/DataScience_Em_Producao/data/train.csv', low_memory=False )\n",
    "df_store_raw = pd.read_csv( 'C:/Users/Michelle/repos/DataScience_Em_Producao/data/store.csv', low_memory=False )\n",
    "\n",
    "# merge\n",
    "df_raw = pd.merge( df_sales_raw, df_store_raw, how='left', on='Store' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0.  Descrição dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_old = ['Store', 'DayOfWeek', 'Date', 'Sales', 'Customers', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday', \n",
    "            'StoreType', 'Assortment', 'CompetitionDistance', 'CompetitionOpenSinceMonth',\n",
    "            'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval']\n",
    "\n",
    "snakecase = lambda x: inflection.underscore( x )\n",
    "\n",
    "cols_new = list( map( snakecase, cols_old ) )\n",
    "\n",
    "# rename\n",
    "df1.columns = cols_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'Number of Rows: {}'.format( df1.shape[0] ) )\n",
    "print( 'Number of Cols: {}'.format( df1.shape[1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Data Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['date'] = pd.to_datetime( df1['date'] )\n",
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.4 Check NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Fillout NA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#competition_distance              2642\n",
    "#competition_open_since_month    323348\n",
    "#competition_open_since_year     323348\n",
    "#promo2_since_week               508031\n",
    "#promo2_since_year               508031\n",
    "#promo_interval                  508031"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#competition_distance        \n",
    "df1['competition_distance'] = df1['competition_distance'].apply( lambda x: 200000.0 if math.isnan( x ) else x )\n",
    "\n",
    "#competition_open_since_month\n",
    "df1['competition_open_since_month'] = df1.apply( lambda x: x['date'].month if math.isnan( x['competition_open_since_month'] ) else x['competition_open_since_month'], axis=1 )\n",
    "\n",
    "#competition_open_since_year \n",
    "df1['competition_open_since_year'] = df1.apply( lambda x: x['date'].year if math.isnan( x['competition_open_since_year'] ) else x['competition_open_since_year'], axis=1 )\n",
    "\n",
    "#promo2_since_week           \n",
    "df1['promo2_since_week'] = df1.apply( lambda x: x['date'].week if math.isnan( x['promo2_since_week'] ) else x['promo2_since_week'], axis=1 )\n",
    "\n",
    "#promo2_since_year           \n",
    "df1['promo2_since_year'] = df1.apply( lambda x: x['date'].year if math.isnan( x['promo2_since_year'] ) else x['promo2_since_year'], axis=1 )\n",
    "\n",
    "#promo_interval              \n",
    "month_map = {1: 'Jan',  2: 'Fev',  3: 'Mar',  4: 'Apr',  5: 'May',  6: 'Jun',  7: 'Jul',  8: 'Aug',  9: 'Sep',  10: 'Oct', 11: 'Nov', 12: 'Dec'}\n",
    "\n",
    "df1['promo_interval'].fillna(0, inplace=True )\n",
    "\n",
    "df1['month_map'] = df1['date'].dt.month.map( month_map )\n",
    "\n",
    "df1['is_promo'] = df1[['promo_interval', 'month_map']].apply( lambda x: 0 if x['promo_interval'] == 0 else 1 if x['month_map'] in x['promo_interval'].split( ',' ) else 0, axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1['is_promo'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1[df1['is_promo']==1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Change dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# competiton\n",
    "df1['competition_open_since_month'] = df1['competition_open_since_month'].astype( int64 )\n",
    "df1['competition_open_since_year'] = df1['competition_open_since_year'].astype( int64 )\n",
    "    \n",
    "# promo2\n",
    "df1['promo2_since_week'] = df1['promo2_since_week'].astype( int64 )\n",
    "df1['promo2_since_year'] = df1['promo2_since_year'].astype( int64 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Descritive Statistical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attributes = df1.select_dtypes( include=['int64', 'float64'] )\n",
    "cat_attributes = df1.select_dtypes( exclude=['int64', 'float64', 'datetime64[ns]'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.1 Numerical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central Tendency - mean, meadina \n",
    "ct1 = pd.DataFrame( num_attributes.apply( np.mean ) ).T\n",
    "ct2 = pd.DataFrame( num_attributes.apply( np.median ) ).T\n",
    "\n",
    "# dispersion - std, min, max, range, skew, kurtosis\n",
    "d1 = pd.DataFrame( num_attributes.apply( np.std ) ).T \n",
    "d2 = pd.DataFrame( num_attributes.apply( min ) ).T \n",
    "d3 = pd.DataFrame( num_attributes.apply( max ) ).T \n",
    "d4 = pd.DataFrame( num_attributes.apply( lambda x: x.max() - x.min() ) ).T \n",
    "d5 = pd.DataFrame( num_attributes.apply( lambda x: x.skew() ) ).T \n",
    "d6 = pd.DataFrame( num_attributes.apply( lambda x: x.kurtosis() ) ).T \n",
    "\n",
    "# concatenar\n",
    "m = pd.concat( [d2, d3, d4, ct1, ct2, d1, d5, d6] ).T.reset_index()\n",
    "m.columns = ['attributes', 'min', 'max', 'range', 'mean', 'median', 'std', 'skew', 'kurtosis']\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot( df1['competition_distance'], kde=False )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.2 Categorical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_attributes.apply( lambda x: x.unique().shape[0] ) #Imprime os tipos de cada variável"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = df1[(df1['state_holiday'] != '0') & (df1['sales'] > 0)]\n",
    "\n",
    "plt.subplot( 1, 3, 1 )\n",
    "sns.boxplot( x='state_holiday', y='sales', data=aux )\n",
    "\n",
    "plt.subplot( 1, 3, 2 )\n",
    "sns.boxplot( x='store_type', y='sales', data=aux )\n",
    "\n",
    "plt.subplot( 1, 3, 3 )\n",
    "sns.boxplot( x='assortment', y='sales', data=aux )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 PASSO 02: FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Mapa Mental de Hipóteses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('img/MindMapHipothesis.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Criação  das Hipóteses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Hipóteses Lojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Lojas com maior quadro de funcionários deveriam vender mais.\n",
    "\n",
    "**2.** Lojas com maior capacidade de estoque deveriam vender mais.\n",
    "\n",
    "**3.** Lojas com maior porte deveriam vender mais.\n",
    "\n",
    "**4.** Lojas com competidores mais próximos deveriam vender menos.\n",
    "\n",
    "**5.** Lojas com competidores a mais tempos deveriam vender mais .\n",
    "\n",
    "**6.** Lojas com maior sortimento deveriam vender mais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2 Hipóteses Produtos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **1.** Lojas que investem mais em Marketing deveriam vender mais.\n",
    "\n",
    "**2.** Lojas que expõe mais os produtos nas vitrines deveriam vender mais.\n",
    "\n",
    "**3.** Lojas que tem preços menores nos produtos deveriam vender mais.\n",
    "\n",
    "**4.** Lojas como promoções mais agressivas (descontos maiores), deveriam vender mais.\n",
    "\n",
    "**5.** Lojas como promoções ativas por mais tempo deveriam vender mais.\n",
    "\n",
    "**6.** Lojas com mais dias de promoção deveriam vender mais \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3. Hipótese Tempo    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Lojas abertas durante o feriado do Natal deveriam vender mais.\n",
    "\n",
    "**2.** Lojas  deveriam vender mais ao longo dos anos.\n",
    "\n",
    "**3.** Lojas  deveriam vender mais no segundo semestre do ano.\n",
    "\n",
    "**4.** Lojas  deveriam vender mais depois do dia 10 de cada mês.\n",
    "\n",
    "**5.** Lojas  deveriam vender menos aos final de semana.\n",
    "\n",
    "**6.** Lojas  deveriam vender menos durante os feriados escolares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Lista final de Hipóteses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Lojas com competidores mais próximos deveriam vender menos.\n",
    "\n",
    "**2.** Lojas com maior sortimento deveriam vender mais.\n",
    "\n",
    "**3.** Lojas com competidores a mais tempos deveriam vender mais.\n",
    "\n",
    "**4.** Lojas como promoções mais agressivas (descontos maiores), deveriam vender mais.\n",
    "\n",
    "**5.** Lojas com promoções ativas por mais tempo deveriam vender mais.\n",
    "\n",
    "**6.** Lojas com mais dias de promoção deveriam vender mais. \n",
    "\n",
    "**7.** Lojas abertas durante o feriado do Natal deveriam vender mais.\n",
    "\n",
    "**8.** Lojas  deveriam vender mais ao longo dos anos.\n",
    "\n",
    "**9.** Lojas  deveriam vender mais no segundo semestre do ano.\n",
    "\n",
    "**10.** Lojas  deveriam vender mais depois do dia 10 de cada mês.\n",
    "\n",
    "**11.** Lojas  deveriam vender menos aos finais de semana.\n",
    "\n",
    "**12.** Lojas  deveriam vender menos durante os feriados escolares.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Feature Enginnering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year\n",
    "df2['year'] = df2['date'].dt.year\n",
    "\n",
    "# month\n",
    "df2['month'] = df2['date'].dt.month\n",
    "\n",
    "# day\n",
    "df2['day'] = df2['date'].dt.day\n",
    "\n",
    "# week of year\n",
    "df2['week_of_year'] = df2['date'].dt.weekofyear\n",
    "\n",
    "# year week\n",
    "df2['year_week'] = df2['date'].dt.strftime( '%Y-%W' )\n",
    "\n",
    "# competition since\n",
    "df2['competition_since'] = df2.apply( lambda x: datetime.datetime( year=x['competition_open_since_year'], month=x['competition_open_since_month'],day=1 ), axis=1 )\n",
    "df2['competition_time_month'] = ( ( df2['date'] - df2['competition_since'] )/30 ).apply( lambda x: x.days ).astype( int )\n",
    "\n",
    "# promo since\n",
    "df2['promo_since'] = df2['promo2_since_year'].astype( str ) + '-' + df2['promo2_since_week'].astype( str )\n",
    "df2['promo_since'] = df2['promo_since'].apply( lambda x: datetime.datetime.strptime( x + '-1', '%Y-%W-%w' ) - datetime.timedelta( days=7 ) )\n",
    "df2['promo_time_week'] = ( ( df2['date'] - df2['promo_since'] )/7 ).apply( lambda x: x.days ).astype( int )\n",
    "\n",
    "# assortment\n",
    "df2['assortment'] = df2['assortment'].apply( lambda x: 'basic' if x == 'a' else 'extra' if x == 'b' else 'extended' )\n",
    "\n",
    "# state holiday\n",
    "df2['state_holiday'] = df2['state_holiday'].apply( lambda x: 'public_holiday' if x == 'a' else 'easter_holiday' if x == 'b' else 'christmas' if x == 'c' else 'regular_day' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Filtragem de Variáveis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Filtragem das Linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3[(df3['open'] != 0) & (df3['sales'] > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Seleção das Colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_drop = ['customers', 'open', 'promo_interval', 'month_map']\n",
    "df3 = df3.drop( cols_drop, axis=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 - ANÁLISE EXPLORATÓRIA DOS DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Análise Univariada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.1. Response Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot( df4['sales'], kde=False  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Numerica Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attributes.hist( bins=25 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Categorical Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state_holiday\n",
    "plt.figure( figsize=(20, 18))\n",
    "plt.subplot(3,2,1)\n",
    "\n",
    "a = df4[df4['state_holiday']!= 'regular_day']\n",
    "sns.countplot(a['state_holiday'])\n",
    "\n",
    "\n",
    "plt.subplot(3,2,2)\n",
    "sns.kdeplot(df4[df4['state_holiday'] == 'public_holiday']['sales'],label ='public_holiday', shade=True)\n",
    "sns.kdeplot(df4[df4['state_holiday'] == 'easter_holiday']['sales'],label ='easter_holiday', shade=True)\n",
    "sns.kdeplot(df4[df4['state_holiday'] == 'christmas']['sales'],label ='christmas', shade=True)\n",
    "\n",
    "#store_type\n",
    "\n",
    "plt.subplot(3,2,3)\n",
    "sns.countplot(df4['store_type']);\n",
    "\n",
    "plt.subplot(3,2,4)\n",
    "sns.kdeplot(df4[df4['store_type'] == 'a']['sales'],label ='a', shade=True);\n",
    "sns.kdeplot(df4[df4['store_type'] == 'b']['sales'],label ='b', shade=True);\n",
    "sns.kdeplot(df4[df4['store_type'] == 'c']['sales'],label ='c', shade=True);\n",
    "sns.kdeplot(df4[df4['store_type'] == 'd']['sales'],label ='d', shade=True);\n",
    "\n",
    "#assortment\n",
    "\n",
    "plt.subplot(3,2,5)\n",
    "sns.countplot(df4['assortment']);\n",
    "\n",
    "plt.subplot(3,2,6)\n",
    "sns.kdeplot(df4[df4['assortment'] == 'basic']['sales'],label ='basic', shade=True);\n",
    "sns.kdeplot(df4[df4['assortment'] == 'extended']['sales'],label ='extended', shade=True);\n",
    "sns.kdeplot(df4[df4['assortment'] == 'extra']['sales'],label ='extra', shade=True);\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Análise Bivariada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H1. Lojas com maior sortimento deveriam vender mais.\n",
    "***False*** Lojas com MAIOR SORTIMENTO vendem MENOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1= df4[['assortment', 'sales']].groupby('assortment').sum().reset_index()\n",
    "sns.barplot( x='assortment', y='sales', data=aux1 );\n",
    "\n",
    "aux2 = df4[['year_week','assortment','sales']].groupby(['year_week','assortment']).sum().reset_index()\n",
    "aux2.pivot(index= 'year_week', columns='assortment', values='sales').plot();\n",
    "\n",
    "aux3 = aux2[aux2['assortment']=='extra']\n",
    "aux3.pivot(index= 'year_week', columns='assortment', values='sales').plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  H2 - Lojas com competidores mais próximos deveriam vender menos.\n",
    "**Falsa** Lojas com competidores mais PRÓXIMOS vendem MAIS\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = df4[['competition_distance', 'sales']].groupby('competition_distance').sum().reset_index()\n",
    "plt.subplot(1,3,1)\n",
    "sns.scatterplot(x ='competition_distance', y='sales', data=aux1)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "bins = list(np.arange(0,20000, 1000))\n",
    "aux1['competition_distance_binned'] = pd.cut(aux1['competition_distance'], bins=bins)\n",
    "aux2 = aux1[['competition_distance_binned', 'sales']].groupby('competition_distance_binned').sum().reset_index()\n",
    "sns.barplot(x='competition_distance_binned', y='sales', data=aux2);\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H3. Lojas com competidores a mais tempo deveriam vender mais.\n",
    "**Falsa** Lojas com COMPETIDORES a MAIS TEMPO vendem MENOS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize=(40, 22))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "aux1 = df4[['competition_time_month', 'sales']].groupby('competition_time_month').sum().reset_index()\n",
    "aux2 = aux1[(aux1['competition_time_month']<120)& (aux1['competition_time_month'] !=0)]\n",
    "sns.barplot(x='competition_time_month', y='sales', data=aux2);\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.regplot(x='competition_time_month', y='sales', data=aux2)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "x=sns.heatmap(aux1.corr(method='pearson'), annot =True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **H4.**  Lojas como promoções ativas por mais tempo deveriam vender mais.\n",
    "**Falsa** Lojas com promoçoes ativas por MAIS TEMPO vendem MENOS, depois de um certo período de promoção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = df4[['promo_time_week', 'sales']].groupby('promo_time_week').sum().reset_index()\n",
    "\n",
    "plt.subplot(2,3,1)\n",
    "aux2 = aux1[aux1['promo_time_week'] > 0]  #promo extendido\n",
    "sns.barplot(x='promo_time_week', y='sales', data=aux2);\n",
    "plt.xticks(rotation =90);\n",
    "\n",
    "plt.subplot(2,3,2)\n",
    "sns.regplot(x='promo_time_week', y='sales', data=aux2);\n",
    "\n",
    "plt.subplot(2,3,3)\n",
    "aux3 = aux1[aux1['promo_time_week'] < 0]  #promo regular \n",
    "sns.barplot(x='promo_time_week', y='sales', data=aux3);\n",
    "plt.xticks(rotation =90);\n",
    "\n",
    "plt.subplot(2,3,4)\n",
    "sns.regplot(x='promo_time_week', y='sales', data=aux3);\n",
    "\n",
    "plt.subplot(2,3,5)\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <s>**H5.** Lojas com mais dias de promoção deveriam vender mais </s>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **H7.** Lojas como promoções consecutivas deveriam vender mais.\n",
    "**Falsa** Lojas como promoções consecutivas vendem menos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[['promo','promo2', 'sales']].groupby(['promo','promo2']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = df4[(df4['promo']==1) & (df4['promo2']==1)][['year_week','sales']].groupby('year_week').sum().reset_index()\n",
    "ax = aux1.plot()\n",
    "\n",
    "aux2 = df4[(df4['promo']==1) & (df4['promo2']==0)][['year_week','sales']].groupby('year_week').sum().reset_index()\n",
    "aux2.plot(ax=ax)\n",
    "\n",
    "ax.legend(labels=['Tradicional & Extendida','Extendida']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **H8.** Lojas abertas durante o feriado do Natal deveriam vender mais.\n",
    "**Falsa** Lojas abertas durante o feriado do Natal vendem menos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize=(20, 10))\n",
    "aux= df4[df4['state_holiday'] != 'regular_day']\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "aux1 = aux[['state_holiday', 'sales']].groupby('state_holiday').sum().reset_index()\n",
    "sns.barplot(x='state_holiday', y='sales', data = aux1);\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "aux2= aux[['year', 'state_holiday','sales']].groupby(['year', 'state_holiday']).sum().reset_index()\n",
    "sns.barplot(x='year', y='sales', hue='state_holiday', data=aux2);   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **H9.** Lojas  deveriam vender mais ao longo dos anos.\n",
    "**Falsa** As lojas vendem menos ao longo dos anos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = df4[['year','sales']].groupby('year').sum().reset_index()\n",
    "plt.figure( figsize=(20, 10))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "sns.barplot(x='year', y='sales', data=aux1);\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.regplot(x='year', y='sales', data=aux1);\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True);\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **H10.** Lojas  deveriam vender mais no segundo semestre do ano.\n",
    "**Falsa** As vendas caem no segundo semestre do ano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = df4[['month','sales']].groupby('month').sum().reset_index()\n",
    "plt.figure( figsize=(20, 10))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "sns.barplot(x='month', y='sales', data=aux1);\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.regplot(x='month', y='sales', data=aux1);\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True);\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **H11.** Lojas  deveriam vender mais depois do dia 10 de cada mês.\n",
    "**Verdadeira** Lojas vendem mais depois dia 10 de cada mês."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = df4[['day','sales']].groupby('day').sum().reset_index()\n",
    "plt.figure( figsize=(20, 10))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "sns.barplot(x='day', y='sales', data=aux1);\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "sns.regplot(x='day', y='sales', data=aux1);\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True);\n",
    "\n",
    "aux1['before_after'] = aux1['day'].apply(lambda x: 'before_10_days' if x<=10 else 'after_10_days')\n",
    "aux2= aux1[['before_after', 'sales']].groupby('before_after').sum().reset_index()\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "sns.barplot(x='before_after', y='sales', data= aux2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **H12.** Lojas  deveriam vender menos aos finais de semana.\n",
    "**Falsa** Lojas vendem menos aos finais de semana.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = df4[['day_of_week','sales']].groupby('day_of_week').sum().reset_index()\n",
    "plt.figure( figsize=(20, 10))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "sns.barplot(x='day_of_week', y='sales', data=aux1);\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.regplot(x='day_of_week', y='sales', data=aux1);\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **H13.** Lojas deveriam vender menos durante os feriados escolares.\n",
    "**Verdadeira** Lojas vendem menos os feriados escolares, exceto nos meses de julho e agosto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1= df4[['school_holiday','sales']].groupby('school_holiday').sum().reset_index()\n",
    "plt.subplot(2,1,1)\n",
    "sns.barplot(x='school_holiday', y='sales', data=aux1);\n",
    "aux2 =df4[['month','school_holiday','sales']].groupby(['month','school_holiday']).sum().reset_index()\n",
    "plt.subplot(2,1,2)\n",
    "sns.barplot(x='month', y='sales', hue='school_holiday', data=aux2);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.1. Resumo das Hipoteses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab =[['Hipoteses', 'Conclusao', 'Relevancia'],\n",
    "      ['H1', 'Falsa', 'Baixa'],  \n",
    "      ['H2', 'Falsa', 'Media'],  \n",
    "      ['H3', 'Falsa', 'Media'],\n",
    "      ['H4', 'Falsa', 'Baixa'],\n",
    "      ['H5', '-', '-'],\n",
    "      ['H7', 'Falsa', 'Baixa'],\n",
    "      ['H8', 'Falsa', 'Media'],\n",
    "      ['H9', 'Falsa', 'Alta'],\n",
    "      ['H10', 'Falsa', 'Alta'],\n",
    "      ['H11', 'Verdadeira', 'Alta'],\n",
    "      ['H12', 'Verdadeira', 'Alta'],\n",
    "      ['H13', 'Verdadeira', 'Baixa'],\n",
    "     ]  \n",
    "print( tabulate( tab, headers='firstrow' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 - Análise Multivariada "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1. Numerical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = num_attributes.corr( method='pearson' )\n",
    "sns.heatmap( correlation, annot=True );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5= df4.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Normalização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = RobustScaler()\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "# competition distance\n",
    "df5['competition_distance'] = rs.fit_transform( df5[['competition_distance']].values )\n",
    "pickle.dump( rs, open( 'parameter/competition_distance_scaler.pkl', 'wb') )\n",
    "\n",
    "# competition time month\n",
    "df5['competition_time_month'] = rs.fit_transform( df5[['competition_time_month']].values )\n",
    "pickle.dump( rs, open( 'parameter/competition_time_month_scaler.pkl', 'wb') )\n",
    "\n",
    "# promo time week\n",
    "df5['promo_time_week'] = mms.fit_transform( df5[['promo_time_week']].values )\n",
    "pickle.dump( rs, open( 'parameter/promo_time_week_scaler.pkl', 'wb') )\n",
    "\n",
    "# year\n",
    "df5['year'] = mms.fit_transform( df5[['year']].values )\n",
    "pickle.dump( mms, open( 'parameter/year_scaler.pkl', 'wb') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Transformação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1. Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_holiday - One Hot Encoding\n",
    "df5 = pd.get_dummies( df5, prefix=['state_holiday'], columns=['state_holiday'] )\n",
    "\n",
    "# store_type - Label Encoding\n",
    "le = LabelEncoder()\n",
    "df5['store_type'] = le.fit_transform( df5['store_type'] )\n",
    "pickle.dump( le, open( 'parameter/store_type_scaler.pkl', 'wb') )\n",
    "\n",
    "# assortment - Ordinal Encoding\n",
    "assortment_dict = {'basic': 1,  'extra': 2, 'extended': 3}\n",
    "df5['assortment'] = df5['assortment'].map( assortment_dict )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2. Response Variable Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5['sales'] = np.log1p( df5['sales'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3. Nature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# day of week\n",
    "df5['day_of_week_sin'] = df5['day_of_week'].apply( lambda x: np.sin( x * ( 2. * np.pi/7 ) ) )\n",
    "df5['day_of_week_cos'] = df5['day_of_week'].apply( lambda x: np.cos( x * ( 2. * np.pi/7 ) ) )\n",
    "\n",
    "# month\n",
    "df5['month_sin'] = df5['month'].apply( lambda x: np.sin( x * ( 2. * np.pi/12 ) ) )\n",
    "df5['month_cos'] = df5['month'].apply( lambda x: np.cos( x * ( 2. * np.pi/12 ) ) )\n",
    "\n",
    "# day \n",
    "df5['day_sin'] = df5['day'].apply( lambda x: np.sin( x * ( 2. * np.pi/30 ) ) )\n",
    "df5['day_cos'] = df5['day'].apply( lambda x: np.cos( x * ( 2. * np.pi/30 ) ) )\n",
    "\n",
    "# week of year\n",
    "df5['week_of_year_sin'] = df5['week_of_year'].apply( lambda x: np.sin( x * ( 2. * np.pi/52 ) ) )\n",
    "df5['week_of_year_cos'] = df5['week_of_year'].apply( lambda x: np.cos( x * ( 2. * np.pi/52 ) ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6=df5.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Split dataframe into training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_drop = ['week_of_year', 'day', 'month', 'day_of_week', 'promo_since', 'competition_since', 'year_week' ]\n",
    "df6 = df6.drop( cols_drop, axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset\n",
    "X_train = df6[df6['date'] < '2015-06-19']\n",
    "y_train = X_train['sales']\n",
    "\n",
    "# test dataset\n",
    "X_test = df6[df6['date'] >= '2015-06-19']\n",
    "y_test = X_test['sales']\n",
    "\n",
    "print( 'Training Min Date: {}'.format( X_train['date'].min() ) )\n",
    "print( 'Training Max Date: {}'.format( X_train['date'].max() ) )\n",
    "\n",
    "print( '\\nTest Min Date: {}'.format( X_test['date'].min() ) )\n",
    "print( 'Test Max Date: {}'.format( X_test['date'].max() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Boruta as Feature Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training and test dataset for Boruta\n",
    "#X_train_n = X_train.drop( ['date', 'sales'], axis=1 ).values\n",
    "#y_train_n = y_train.values.ravel()\n",
    "#\n",
    "## define RandomForestRegressor\n",
    "#rf = RandomForestRegressor( n_jobs=-1 )\n",
    "#\n",
    "## define Boruta\n",
    "#boruta = BorutaPy( rf, n_estimators='auto', verbose=2, random_state=42 ).fit( X_train_n, y_train_n )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.1. Best Features from Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols_selected = boruta.support_.tolist()\n",
    "#\n",
    "## best features\n",
    "#X_train_fs = X_train.drop( ['date', 'sales'], axis=1 )\n",
    "#cols_selected_boruta = X_train_fs.iloc[:, cols_selected].columns.to_list()\n",
    "#\n",
    "## not selected boruta\n",
    "#cols_not_selected_boruta = list( np.setdiff1d( X_train_fs.columns, cols_selected_boruta ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Manual Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_selected_boruta = [\n",
    "    'store',\n",
    "    'promo',\n",
    "    'store_type',\n",
    "    'assortment',\n",
    "    'competition_distance',\n",
    "    'competition_open_since_month',\n",
    "    'competition_open_since_year',\n",
    "    'promo2',\n",
    "    'promo2_since_week',\n",
    "    'promo2_since_year',\n",
    "    'competition_time_month',\n",
    "    'promo_time_week',\n",
    "    'day_of_week_sin',\n",
    "    'day_of_week_cos',\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'day_sin',\n",
    "    'day_cos',\n",
    "    'week_of_year_sin',\n",
    "    'week_of_year_cos']\n",
    "\n",
    "# columns to add\n",
    "feat_to_add = ['date', 'sales']\n",
    "\n",
    "cols_selected_boruta_full = cols_selected_boruta.copy()\n",
    "cols_selected_boruta_full.extend( feat_to_add )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0. Machine Learning Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train[ cols_selected_boruta ] #Todas as variáveis escolhidas pelo Boruta sem date e sales\n",
    "x_test = X_test[ cols_selected_boruta ]\n",
    "\n",
    "# Time Series Data Preparation\n",
    "x_training = X_train[ cols_selected_boruta_full ] #Todas as variáveis escolhidas pelo Boruta com date e sales\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.1. Average Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aux1 = x_test.copy()\n",
    "#aux1['sales'] = y_test.copy()\n",
    "\n",
    "#prediction \n",
    "#aux2 = aux1[['store','sales']].groupby('store').mean().reset_index().rename(columns={'sales': 'predictions'})\n",
    "#aux1 = pd.merge( aux1,aux2, how='left', on='store')\n",
    "#yhat_baseline = aux1['predictions']\n",
    "\n",
    "#performance \n",
    "#baseline_result = ml_error ('Average Model', np.expm1(y_test), np.expm1(yhat_baseline))\n",
    "#baseline_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model \n",
    "#lr=LinearRegression().fit(x_train,y_train)\n",
    "\n",
    "#prediction\n",
    "#yhat_lr = lr.predict(x_test)\n",
    "\n",
    "#performance\n",
    "#lr_result = ml_error('Linear Regression', np.expm1(y_test), np.expm1(yhat_lr) )\n",
    "#lr_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1 Linear Regression Model - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_result_cv = cross_validation(x_training, 5, 'Linear Regression', lr, verbose=False)\n",
    "#lr_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. Linear Regression Regularized Model - Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model \n",
    "#lrr=Lasso(alpha=0.01).fit(x_train,y_train)\n",
    "\n",
    "#prediction\n",
    "#yhat_lrr = lrr.predict(x_test)\n",
    "\n",
    "#performance\n",
    "#lrr_result = ml_error('Linear Regression - Lasso', np.expm1(y_test), np.expm1(yhat_lrr) )\n",
    "#lrr_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.1 Linear Regression Regularized Model - Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lrr_result_cv = cross_validation(x_training, 5, 'Lasso', lrr, verbose=False)\n",
    "#lrr_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4. Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model \n",
    "#rf = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=42 ).fit(x_train,y_train)\n",
    "\n",
    "#prediction\n",
    "#yhat_rf = rf.predict(x_test)\n",
    "\n",
    "#performance\n",
    "#rf_result = ml_error('Random Forest Regressor', np.expm1(y_test), np.expm1(yhat_rf) )\n",
    "#rf_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###7.4.1 Random Forest Regressor - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf_result_cv = cross_validation(x_training, 5, 'Random Forest Regressor', rf, verbose=False)\n",
    "#rf_result_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##7.5. XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model \n",
    "#model_xgb = xgb.XGBRegressor( objective= 'reg:squarederror',\n",
    "                              #n_estimators=100,\n",
    "                              #eta=0.01,\n",
    "                              #max_depth=10,\n",
    "                              #colsample_bytree=0.9 ).fit(x_train,y_train)\n",
    "\n",
    "#prediction\n",
    "#yhat_xgb = model_xgb.predict(x_test)\n",
    "\n",
    "#performance\n",
    "#xgb_result = ml_error('XGBoost Regressor', np.expm1(y_test), np.expm1(yhat_xgb) )\n",
    "#xgb_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###7.5.1 XGBoost Regressor - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb_result_cv = cross_validation(x_training, 5, 'XGBoost Regressor', model_xgb, verbose=True)\n",
    "#xgb_result_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##7.6. Compare Model's Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###7.6.1. Single Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelling_result = pd.concat([baseline_result, lr_result, lrr_result,rf_result, xgb_result])\n",
    "#modelling_result.sort_values('RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###7.6.2 Real Performances - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelling_result_cv = pd.concat([lr_result_cv, lrr_result_cv, rf_result_cv, xgb_result_cv])\n",
    "#modelling_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.0. PASSO 08 - HYPERPARAMETER FINE TUNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param = {\n",
    "    #'n_estimators': [1500,2700,2000,2500,3000,3500],\n",
    "    #'eta': [0.01, 0.03],\n",
    "    #'max_depth': [3,5,9],\n",
    "    #'subsample': [0.1, 0.5, 0.7],\n",
    "    #'colsample_bytree': [0.3, 0.7, 0.9],\n",
    "    #'min_child_weight': [3, 8, 15]}\n",
    "#MAX_EVAL=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_result=pd.DataFrame()\n",
    "#for i in range(MAX_EVAL):\n",
    "  #choose values for parameters randomly\n",
    "  #hp= {k:random.sample(v,1)[0] for k, v in param.items()}\n",
    "  #print(hp)\n",
    "\n",
    "  #model\n",
    "  #model_xgb = xgb.XGBRegressor( objective= 'reg:squarederror',\n",
    "                                #n_estimators=hp['n_estimators'],\n",
    "                                #eta=hp['eta'],\n",
    "                                #max_depth=hp['max_depth'],\n",
    "                                #subsample =hp['subsample'],\n",
    "                                #colsample_bytree =hp['colsample_bytree'],\n",
    "                                #min_child_weight=hp['min_child_weight'] )\n",
    "  #performance\n",
    "  #result = cross_validation(x_training, 5, 'XGBoost Regressor',model_xgb, verbose=False )\n",
    "  #final_result = pd.concat([final_result, result])\n",
    "\n",
    "#final_result   \n",
    "\n",
    "                              \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param_tuned = {\n",
    "    #'n_estimators': 3500,\n",
    "    #'max_depth':9 ,\n",
    "    #'subsample':  0.7,\n",
    "    #'colsample_bytree': 0.9,\n",
    "    #'min_child_weight': 8}\n",
    "        \n",
    "#{'n_estimators': 3500, 'eta': 0.03, 'max_depth': 9, 'subsample': 0.7, 'colsample_bytree': 0.9, 'min_child_weight': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "#model_xgb_tuned = xgb.XGBRegressor( objective='reg:squarederror',\n",
    "                                    #n_estimators=param_tuned['n_estimators'], \n",
    "                                    #eta=param_tuned['eta'], \n",
    "                                    #max_depth=param_tuned['max_depth'], \n",
    "                                    #subsample=param_tuned['subsample'],\n",
    "                                    #colsample_bytree=param_tuned['colsample_bytree'],\n",
    "                                    #min_child_weight=param_tuned['min_child_weight'] ).fit( x_train, y_train )\n",
    "\n",
    "# prediction\n",
    "#yhat_xgb_tuned = model_xgb_tuned.predict( x_test )\n",
    "\n",
    "# performance\n",
    "#xgb_result_tuned = ml_error( 'XGBoost Regressor', np.expm1( y_test ), np.expm1( yhat_xgb_tuned ) )\n",
    "#xgb_result_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump( model_xgb_tuned, open( 'C:/Users/Michelle/repos/DataScience_Em_Producao/model/model_rossmann.pkl', 'wb' ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mpe = mean_percentage_error( np.expm1( y_test ), np.expm1( yhat_xgb_tuned ) )\n",
    "#mpe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.0. PASSO 09 - TRADUCAO E INTERPRETACAO DO ERRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = X_test[ cols_selected_boruta_full ]\n",
    "\n",
    "# rescale\n",
    "df9['sales'] = np.expm1( df9['sales'] )\n",
    "df9['predictions'] = np.expm1( yhat_xgb_tuned )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. Business Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of predictions\n",
    "df91 = df9[['store', 'predictions']].groupby( 'store' ).sum().reset_index()\n",
    "\n",
    "# MAE and MAPE\n",
    "df9_aux1 = df9[['store', 'sales', 'predictions']].groupby( 'store' ).apply( lambda x: mean_absolute_error( x['sales'], x['predictions'] ) ).reset_index().rename( columns={0:'MAE'})\n",
    "df9_aux2 = df9[['store', 'sales', 'predictions']].groupby( 'store' ).apply( lambda x: mean_absolute_percentage_error( x['sales'], x['predictions'] ) ).reset_index().rename( columns={0:'MAPE'})\n",
    "\n",
    "# Merge\n",
    "df9_aux3 = pd.merge( df9_aux1, df9_aux2, how='inner', on='store' )\n",
    "df92 = pd.merge( df91, df9_aux3, how='inner', on='store' )\n",
    "\n",
    "# Scenarios\n",
    "df92['worst_scenario'] = df92['predictions'] - df92['MAE']\n",
    "df92['best_scenario'] = df92['predictions'] + df92['MAE']\n",
    "\n",
    "# order columns\n",
    "df92 = df92[['store', 'predictions', 'worst_scenario', 'best_scenario', 'MAE', 'MAPE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df92.sort_values( 'MAPE', ascending=False ).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot( x='store', y='MAPE', data=df92 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2.Total Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df93 = df92[['predictions', 'worst_scenario', 'best_scenario']].apply( lambda x: np.sum( x ), axis=0 ).reset_index().rename( columns={'index': 'Scenario', 0:'Values'} )\n",
    "df93['Values'] = df93['Values'].map( 'R${:,.2f}'.format )\n",
    "df93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3. Machine Learning Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9['error'] = df9['sales'] - df9['predictions']\n",
    "df9['error_rate'] = df9['predictions'] / df9['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot( 2, 2, 1 )\n",
    "sns.lineplot( x='date', y='sales', data=df9, label='SALES' )\n",
    "sns.lineplot( x='date', y='predictions', data=df9, label='PREDICTIONS' )\n",
    "\n",
    "plt.subplot( 2, 2, 2 )\n",
    "sns.lineplot( x='date', y='error_rate', data=df9 )\n",
    "plt.axhline( 1, linestyle='--')\n",
    "\n",
    "plt.subplot( 2, 2, 3 )\n",
    "sns.distplot( df9['error'] )\n",
    "\n",
    "plt.subplot( 2, 2, 4 )\n",
    "sns.scatterplot( df9['predictions'], df9['error'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.0. Deploy Model To Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1. Rossmann Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import inflection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "class Rossmann( object ):\n",
    "    def __init__( self ):\n",
    "        self.home_path='C:/Users/Michelle/repos/DataScience_Em_Producao/'\n",
    "        self.competition_distance_scaler   = pickle.load( open( self.home_path + 'parameter/competition_distance_scaler.pkl', 'rb') )\n",
    "        self.competition_time_month_scaler = pickle.load( open( self.home_path + 'parameter/competition_time_month_scaler.pkl', 'rb') )\n",
    "        self.promo_time_week_scaler        = pickle.load( open( self.home_path + 'parameter/promo_time_week_scaler.pkl', 'rb') )\n",
    "        self.year_scaler                   = pickle.load( open( self.home_path + 'parameter/year_scaler.pkl', 'rb') )\n",
    "        self.store_type_scaler             = pickle.load( open( self.home_path + 'parameter/store_type_scaler.pkl', 'rb') )\n",
    "        \n",
    "        \n",
    "    def data_cleaning( self, df1 ): \n",
    "        \n",
    "        ## 1.1. Rename Columns\n",
    "        cols_old = ['Store', 'DayOfWeek', 'Date', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday', \n",
    "                    'StoreType', 'Assortment', 'CompetitionDistance', 'CompetitionOpenSinceMonth',\n",
    "                    'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval']\n",
    "\n",
    "        snakecase = lambda x: inflection.underscore( x )\n",
    "\n",
    "        cols_new = list( map( snakecase, cols_old ) )\n",
    "\n",
    "        # rename\n",
    "        df1.columns = cols_new\n",
    "\n",
    "        ## 1.3. Data Types\n",
    "        df1['date'] = pd.to_datetime( df1['date'] )\n",
    "\n",
    "        ## 1.5. Fillout NA\n",
    "        #competition_distance        \n",
    "        df1['competition_distance'] = df1['competition_distance'].apply( lambda x: 200000.0 if math.isnan( x ) else x )\n",
    "\n",
    "        #competition_open_since_month\n",
    "        df1['competition_open_since_month'] = df1.apply( lambda x: x['date'].month if math.isnan( x['competition_open_since_month'] ) else x['competition_open_since_month'], axis=1 )\n",
    "\n",
    "        #competition_open_since_year \n",
    "        df1['competition_open_since_year'] = df1.apply( lambda x: x['date'].year if math.isnan( x['competition_open_since_year'] ) else x['competition_open_since_year'], axis=1 )\n",
    "\n",
    "        #promo2_since_week           \n",
    "        df1['promo2_since_week'] = df1.apply( lambda x: x['date'].week if math.isnan( x['promo2_since_week'] ) else x['promo2_since_week'], axis=1 )\n",
    "\n",
    "        #promo2_since_year           \n",
    "        df1['promo2_since_year'] = df1.apply( lambda x: x['date'].year if math.isnan( x['promo2_since_year'] ) else x['promo2_since_year'], axis=1 )\n",
    "\n",
    "        #promo_interval              \n",
    "        month_map = {1: 'Jan',  2: 'Fev',  3: 'Mar',  4: 'Apr',  5: 'May',  6: 'Jun',  7: 'Jul',  8: 'Aug',  9: 'Sep',  10: 'Oct', 11: 'Nov', 12: 'Dec'}\n",
    "\n",
    "        df1['promo_interval'].fillna(0, inplace=True )\n",
    "\n",
    "        df1['month_map'] = df1['date'].dt.month.map( month_map )\n",
    "\n",
    "        df1['is_promo'] = df1[['promo_interval', 'month_map']].apply( lambda x: 0 if x['promo_interval'] == 0 else 1 if x['month_map'] in x['promo_interval'].split( ',' ) else 0, axis=1 )\n",
    "\n",
    "        ## 1.6. Change Data Types\n",
    "        # competiton\n",
    "        df1['competition_open_since_month'] = df1['competition_open_since_month'].astype( int )\n",
    "        df1['competition_open_since_year'] = df1['competition_open_since_year'].astype( int )\n",
    "\n",
    "        # promo2\n",
    "        df1['promo2_since_week'] = df1['promo2_since_week'].astype( int )\n",
    "        df1['promo2_since_year'] = df1['promo2_since_year'].astype( int )\n",
    "        \n",
    "        return df1 \n",
    "\n",
    "\n",
    "    def feature_engineering( self, df2 ):\n",
    "\n",
    "        # year\n",
    "        df2['year'] = df2['date'].dt.year\n",
    "\n",
    "        # month\n",
    "        df2['month'] = df2['date'].dt.month\n",
    "\n",
    "        # day\n",
    "        df2['day'] = df2['date'].dt.day\n",
    "\n",
    "        # week of year\n",
    "        df2['week_of_year'] = df2['date'].dt.weekofyear\n",
    "\n",
    "        # year week\n",
    "        df2['year_week'] = df2['date'].dt.strftime( '%Y-%W' )\n",
    "\n",
    "        # competition since\n",
    "        df2['competition_since'] = df2.apply( lambda x: datetime.datetime( year=x['competition_open_since_year'], month=x['competition_open_since_month'],day=1 ), axis=1 )\n",
    "        df2['competition_time_month'] = ( ( df2['date'] - df2['competition_since'] )/30 ).apply( lambda x: x.days ).astype( int )\n",
    "\n",
    "        # promo since\n",
    "        df2['promo_since'] = df2['promo2_since_year'].astype( str ) + '-' + df2['promo2_since_week'].astype( str )\n",
    "        df2['promo_since'] = df2['promo_since'].apply( lambda x: datetime.datetime.strptime( x + '-1', '%Y-%W-%w' ) - datetime.timedelta( days=7 ) )\n",
    "        df2['promo_time_week'] = ( ( df2['date'] - df2['promo_since'] )/7 ).apply( lambda x: x.days ).astype( int )\n",
    "\n",
    "        # assortment\n",
    "        df2['assortment'] = df2['assortment'].apply( lambda x: 'basic' if x == 'a' else 'extra' if x == 'b' else 'extended' )\n",
    "\n",
    "        # state holiday\n",
    "        df2['state_holiday'] = df2['state_holiday'].apply( lambda x: 'public_holiday' if x == 'a' else 'easter_holiday' if x == 'b' else 'christmas' if x == 'c' else 'regular_day' )\n",
    "\n",
    "        # 3.0. PASSO 03 - FILTRAGEM DE VARIÁVEIS\n",
    "        ## 3.1. Filtragem das Linhas\n",
    "        df2 = df2[df2['open'] != 0]\n",
    "\n",
    "        ## 3.2. Selecao das Colunas\n",
    "        cols_drop = ['open', 'promo_interval', 'month_map']\n",
    "        df2 = df2.drop( cols_drop, axis=1 )\n",
    "        \n",
    "        return df2\n",
    "\n",
    "\n",
    "    def data_preparation( self, df5 ):\n",
    "\n",
    "        ## 5.2. Rescaling \n",
    "        # competition distance\n",
    "        df5['competition_distance'] = self.competition_distance_scaler.fit_transform( df5[['competition_distance']].values )\n",
    "    \n",
    "        # competition time month\n",
    "        df5['competition_time_month'] = self.competition_time_month_scaler.fit_transform( df5[['competition_time_month']].values )\n",
    "\n",
    "        # promo time week\n",
    "        df5['promo_time_week'] = self.promo_time_week_scaler.fit_transform( df5[['promo_time_week']].values )\n",
    "        \n",
    "        # year\n",
    "        df5['year'] = self.year_scaler.fit_transform( df5[['year']].values )\n",
    "\n",
    "        ### 5.3.1. Encoding\n",
    "        # state_holiday - One Hot Encoding\n",
    "        df5 = pd.get_dummies( df5, prefix=['state_holiday'], columns=['state_holiday'] )\n",
    "\n",
    "        # store_type - Label Encoding\n",
    "        df5['store_type'] = self.store_type_scaler.fit_transform( df5['store_type'] )\n",
    "\n",
    "        # assortment - Ordinal Encoding\n",
    "        assortment_dict = {'basic': 1,  'extra': 2, 'extended': 3}\n",
    "        df5['assortment'] = df5['assortment'].map( assortment_dict )\n",
    "\n",
    "        \n",
    "        ### 5.3.3. Nature Transformation\n",
    "        # day of week\n",
    "        df5['day_of_week_sin'] = df5['day_of_week'].apply( lambda x: np.sin( x * ( 2. * np.pi/7 ) ) )\n",
    "        df5['day_of_week_cos'] = df5['day_of_week'].apply( lambda x: np.cos( x * ( 2. * np.pi/7 ) ) )\n",
    "\n",
    "        # month\n",
    "        df5['month_sin'] = df5['month'].apply( lambda x: np.sin( x * ( 2. * np.pi/12 ) ) )\n",
    "        df5['month_cos'] = df5['month'].apply( lambda x: np.cos( x * ( 2. * np.pi/12 ) ) )\n",
    "\n",
    "        # day \n",
    "        df5['day_sin'] = df5['day'].apply( lambda x: np.sin( x * ( 2. * np.pi/30 ) ) )\n",
    "        df5['day_cos'] = df5['day'].apply( lambda x: np.cos( x * ( 2. * np.pi/30 ) ) )\n",
    "\n",
    "        # week of year\n",
    "        df5['week_of_year_sin'] = df5['week_of_year'].apply( lambda x: np.sin( x * ( 2. * np.pi/52 ) ) )\n",
    "        df5['week_of_year_cos'] = df5['week_of_year'].apply( lambda x: np.cos( x * ( 2. * np.pi/52 ) ) )\n",
    "        \n",
    "        \n",
    "        cols_selected = [ 'store', 'promo', 'store_type', 'assortment', 'competition_distance', 'competition_open_since_month',\n",
    "            'competition_open_since_year', 'promo2', 'promo2_since_week', 'promo2_since_year', 'competition_time_month', 'promo_time_week',\n",
    "            'day_of_week_sin', 'day_of_week_cos', 'month_sin', 'month_cos', 'day_sin', 'day_cos', 'week_of_year_sin', 'week_of_year_cos']\n",
    "        \n",
    "        return df5[ cols_selected ]\n",
    "    \n",
    "    \n",
    "    def get_prediction( self, model, original_data, test_data ):\n",
    "        # prediction\n",
    "        pred = model.predict( test_data )\n",
    "        \n",
    "        # join pred into the original data\n",
    "        original_data['prediction'] = np.expm1( pred )\n",
    "        \n",
    "        return original_data.to_json( orient='records', date_format='iso' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2. API Handler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rossmann'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-fe2a8668a8cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mflask\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mFlask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mResponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mrossmann\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRossmann\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRossmann\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# loading model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rossmann'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from flask             import Flask, request, Response\n",
    "from rossmann.Rossmann import Rossmann\n",
    "\n",
    "# loading model\n",
    "model = pickle.load( open( 'C:/Users/Michelle/repos/DataScience_Em_Producao/model/model_rossmann.pkl', 'rb') )\n",
    "\n",
    "# initialize API\n",
    "app = Flask( __name__ )\n",
    "\n",
    "@app.route( '/rossmann/predict', methods=['POST'] )\n",
    "def rossmann_predict():\n",
    "    test_json = request.get_json()\n",
    "   \n",
    "    if test_json: # there is data\n",
    "        if isinstance( test_json, dict ): # unique example\n",
    "            test_raw = pd.DataFrame( test_json, index=[0] )\n",
    "            \n",
    "        else: # multiple example\n",
    "            test_raw = pd.DataFrame( test_json, columns=test_json[0].keys() )\n",
    "            \n",
    "        # Instantiate Rossmann class\n",
    "        pipeline = Rossmann()\n",
    "        \n",
    "        # data cleaning\n",
    "        df1 = pipeline.data_cleaning( test_raw )\n",
    "        \n",
    "        # feature engineering\n",
    "        df2 = pipeline.feature_engineering( df1 )\n",
    "        \n",
    "        # data preparation\n",
    "        df3 = pipeline.data_preparation( df2 )\n",
    "        \n",
    "        # prediction\n",
    "        df_response = pipeline.get_prediction( model, test_raw, df3 )\n",
    "        \n",
    "        return df_response\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        return Reponse( '{}', status=200, mimetype='application/json' )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run( '0.0.0.0' )\n",
    "    #app.run( '127.0.0.1' )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask             import Flask, request, Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10 = pd.read_csv('C:/Users/Michelle/repos/DataScience_Em_Producao/data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge test dataset + store\n",
    "df_test = pd.merge( df10, df_store_raw, how='left', on='Store' )\n",
    "\n",
    "# choose store for prediction\n",
    "df_test = df_test[df_test['Store'].isin( [20, 23, 22] )]\n",
    "\n",
    "# remove closed days\n",
    "df_test = df_test[df_test['Open'] != 0]\n",
    "df_test = df_test[~df_test['Open'].isnull()]\n",
    "df_test = df_test.drop( 'Id', axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Dataframe to json\n",
    "data = json.dumps( df_test.to_dict( orient='records' ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code 503\n"
     ]
    }
   ],
   "source": [
    "#Api Call\n",
    "\n",
    "#url = 'http://127.0.0.1:5000/rossmann/predict' \n",
    "url = 'https://prev-vendas.herokuapp.com/rossmann/predict'\n",
    "header = {'Content-type': 'application/json' }\n",
    "data = data\n",
    "r = requests.post( url, data=data, headers=header )\n",
    "print( 'Status Code {}'.format( r.status_code ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1=pd.DataFrame(r.json(), columns=r.json()[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = d1[['store', 'prediction']].groupby( 'store' ).sum().reset_index()\n",
    "\n",
    "for i in range( len( d2 ) ):\n",
    "    print( 'Store Number {} will sell R${:,.2f} in the next 6 weeks'.format( \n",
    "            d2.loc[i, 'store'], \n",
    "            d2.loc[i, 'prediction'] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
